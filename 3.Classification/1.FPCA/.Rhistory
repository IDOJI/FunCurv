results_list[["hyper-para_alpha"]] = alpha
results_list[["hyper-para_lambda"]] = lambda
# Model fitting
results_list[["fitted_model"]] = fit = glmnet(x = as.matrix(train_X), y = train_y,
family = "binomial", alpha = alpha,
lambda = lambda)
results_list[["pred_prob"]] = pred_prob = as.numeric(predict(fit, newx = as.matrix(test_X),
type = "response"))
# Check if all predicted probabilities are the same
if (length(unique(pred_prob)) != 1) {
# Calculate AUC
results_list[["ROC_curve"]] = roc_curve = roc(test_y, pred_prob, quiet = T)
results_list[["ROC_AUC"]] = auc_value = auc(roc_curve) %>% unlist %>% as.numeric
# Calculate PR-AUC
results_list[["PR_curve"]] = pr_curve = pr.curve(scores.class0 = pred_prob[test_y == 1],
scores.class1 = pred_prob[test_y == 0],
curve = TRUE)
results_list[["PR_AUC"]] = pr_curve$auc.integral
# Convert probabilities to binary predictions using 0.5 threshold
pred_class = ifelse(pred_prob >= 0.5, 1, 0)
# Calculate Precision, Recall, and F1 Score
tp = sum(pred_class == 1 & test_y == 1)
fp = sum(pred_class == 1 & test_y == 0)
fn = sum(pred_class == 0 & test_y == 1)
precision = ifelse((tp + fp) > 0, tp / (tp + fp), 0)
recall = ifelse((tp + fn) > 0, tp / (tp + fn), 0)
f1_score = ifelse((precision + recall) > 0, 2 * (precision * recall) / (precision + recall), NA)
results_list[["F1_Score"]] = f1_score
# ROC Curve ìº¡ì²˜í•˜ì—¬ ì €ì¥
if(plotting){
plot(roc_curve, main = "ROC Curve")
results_list[["ROC_curve_plot"]] = recordPlot()  # í˜„ì¬ plotì„ ìº¡ì²˜í•˜ì—¬ ì €ì¥
# PR Curve ìº¡ì²˜í•˜ì—¬ ì €ì¥
plot(pr_curve, main = "Precision-Recall Curve")
results_list[["PR_curve_plot"]] = recordPlot()   # í˜„ì¬ plotì„ ìº¡ì²˜í•˜ì—¬ ì €ì¥
}
}else{
results_list[["ROC_curve"]] = NA
results_list[["ROC_AUC"]] = NA
results_list[["PR_curve"]] = NA
results_list[["PR_AUC"]] = NA
results_list[["F1_Score"]] = NA
}
return(results_list)
}
# ğŸŸ© Group penalty logistic regression =========================================================================================================
group_penalized_logistic = function(train_X, train_y, test_X, test_y, groups,
lambda = NULL, alpha,
plotting = FALSE){
# Save results
results_list = list()
results_list[["y_real"]] = test_y
results_list[["hyper-para_lambda"]] = lambda
# Model fitting using grpreg
if(is.null(lambda)){
results_list[["fitted_model"]] = fit = grpreg(X = as.matrix(train_X),
y = train_y,
group = groups,
penalty = "grLasso",
family = "binomial",
alpha = alpha)
}else{
results_list[["fitted_model"]] = fit = grpreg(X = as.matrix(train_X),
y = train_y,
group = groups,
penalty = "grLasso",
family = "binomial",
lambda = lambda,
alpha = alpha)
}
# Prediction on test set
results_list[["pred_prob"]] = pred_prob = as.numeric(predict(fit, as.matrix(test_X), type = "response"))
# Check if all predicted probabilities are the same
if (length(unique(pred_prob)) != 1) {
# Calculate AUC
results_list[["ROC_curve"]] = roc_curve = roc(test_y, pred_prob, quiet = TRUE)
results_list[["ROC_AUC"]] = auc_value = as.numeric(auc(roc_curve))
# Calculate PR-AUC
results_list[["PR_curve"]] = pr_curve = pr.curve(scores.class0 = pred_prob[test_y == 1],
scores.class1 = pred_prob[test_y == 0],
curve = TRUE)
results_list[["PR_AUC"]] = pr_curve$auc.integral
# Convert probabilities to binary predictions using 0.5 threshold
pred_class = ifelse(pred_prob >= 0.5, 1, 0)
# Calculate Precision, Recall, and F1 Score
tp = sum(pred_class == 1 & test_y == 1)
fp = sum(pred_class == 1 & test_y == 0)
fn = sum(pred_class == 0 & test_y == 1)
precision = ifelse((tp + fp) > 0, tp / (tp + fp), 0)
recall = ifelse((tp + fn) > 0, tp / (tp + fn), 0)
f1_score = ifelse((precision + recall) > 0, 2 * (precision * recall) / (precision + recall), NA)
results_list[["F1_Score"]] = f1_score
# ROC Curve capture
if(plotting){
plot(roc_curve, main = "ROC Curve")
results_list[["ROC_curve_plot"]] = recordPlot()  # Store ROC plot
# PR Curve capture
plot(pr_curve, main = "Precision-Recall Curve")
results_list[["PR_curve_plot"]] = recordPlot()   # Store PR plot
}
} else {
# Assign NA if all predicted probabilities are the same
results_list[["ROC_curve"]] = NA
results_list[["ROC_AUC"]] = NA
results_list[["PR_curve"]] = NA
results_list[["PR_AUC"]] = NA
results_list[["F1_Score"]] = NA
}
return(results_list)
}
# ğŸŸª Grid and Fold ======================================================================================================
penalized_logistic_grid = function(train_X, train_y, groups = NULL, alphas, lambdas = NULL, test_X, test_y, plotting = FALSE) {
library(crayon)
results = list()
for (alpha in alphas) {
# alpha = alphas[1]
for (lambda in lambdas %||% c(NULL)) {  # lambdasê°€ NULLì¼ ë•Œ NAë¡œ ì„¤ì •
# lambda = lambdas[1]
# ê° ì¡°í•©ì„ ê³ ìœ í•œ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
param_name = if (is.null(lambda)) {
sprintf("alpha_%.4f_lambda_NULL", round(alpha, 4))
} else {
sprintf("alpha_%.4f_lambda_%.4f", round(alpha, 4), round(lambda, 4))
}
# ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì‹œì‘
start_time = Sys.time()
# ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ í˜„ì¬ alphaì™€ lambda ê°’ì„ ì¶œë ¥í•˜ê³  í•¨ìˆ˜ ì¤‘ë‹¨
tryCatch({
# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì— ëŒ€í•´ penalized_logistic í•¨ìˆ˜ í˜¸ì¶œ
if (is.null(groups)) {
results[[param_name]] = penalized_logistic(
train_X = train_X,
train_y = train_y,
test_X = test_X,
test_y = test_y,
alpha = alpha,
lambda = lambda,
plotting = plotting
)
} else {
# lambdaê°€ NULLì¼ ë•Œ, group_penalized_logisticì—ì„œ ìë™ ì„¤ì •
results[[param_name]] = group_penalized_logistic(
train_X = train_X,
train_y = train_y,
test_X = test_X,
test_y = test_y,
groups = groups,
alpha = alpha,
lambda = if (is.null(lambda)) NULL else lambda,
plotting = plotting
)
}
# ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
end_time = Sys.time()
elapsed_time = round(as.numeric(difftime(end_time, start_time, units = "secs")), 2)
# ê²°ê³¼ ë©”ì‹œì§€ ì¶œë ¥
cat(green("Completed"), "with alpha =", yellow(alpha),
if (!is.null(lambda)) paste("and lambda =", yellow(lambda)),
"in", blue(elapsed_time), "seconds.\n")
}, error = function(e) {
cat(red("Error occurred with alpha =", alpha),
if (!is.null(lambda)) paste("and lambda =", lambda), "\n")
print(e)  # ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
results[[param_name]] = NA
})
}
}
return(results)
}
penalized_logistic_grid_fold = function(path_data,
path_splitted_subjects,
group_penalty = TRUE,
alphas,
lambdas,
path_save = NULL){
# Define final results save path
path_final_results = file.path(path_save, paste0(basename(path_data),
"___Classification",
ifelse(group_penalty, "_GroupPen", "_Pen"),
".rds"))
# Check if final results file already exists; if so, load and return
if (file.exists(path_final_results)) {
cat(green("Final results file already exists. Loading saved results.\n"))
return(readRDS(path_final_results))
}
target_groups = strsplit(basename(path_data), "___")[[1]][1]
path_fold = list.files(path_data, pattern = "Fold_", full.names = TRUE)
# Load subjects list
subjects = file.path(path_splitted_subjects, target_groups) %>%
list.files(pattern = "train_seed", full.names = TRUE) %>%
readRDS()
fold_results = list()
path_fold_save = file.path(path_save, basename(path_data))
dir.create(path_fold_save, showWarnings = FALSE, recursive = TRUE)
# Model fitting
for (k in seq_along(path_fold)) {
# Define the save path for each fold result
path_fold_save_file = file.path(path_fold_save, paste0(k, "th_fold.rds"))
# Check if fold result file exists; if so, load and skip
if (file.exists(path_fold_save_file)) {
fold_results[[paste0(k, "th_fold")]] = readRDS(path_fold_save_file)
cat(green(sprintf("Fold %d already exists. Loaded saved results.\n", k)))
next  # Skip to the next iteration
}
# Sub list
sub_train = subjects[[paste0("Fold_", k, "_Train")]]
sub_valid = subjects[[paste0("Fold_", k, "_Validation")]]
# Extract Diagnosis
diagnosis = extract_dignosis(
sub_train = sub_train,
sub_test = sub_valid,
which_group_positive = ifelse(grepl("AD", target_groups), "Dementia",
ifelse(grepl("MCI", target_groups), "MCI", "CN"))
)
# Extract PC scores
kth_fold_scores = extract_fpca_scores_and_group_numbers(path_fold[k])
# Define groups if group_penalty is enabled
groups_return = function(group_penalty, group_numbers) {
if (group_penalty) return(group_numbers) else return(NULL)
}
# Logistic regression with penalized grid search
fold_results[[paste0(k, "th_fold")]] = penalized_logistic_grid(
train_X = kth_fold_scores$train_scores %>% dplyr::select(-RID) %>% as.matrix(),
train_y = diagnosis$diagnosis_train,
alphas = alphas,
lambdas = lambdas,
test_X = kth_fold_scores$test_scores %>% dplyr::select(-RID) %>% as.matrix(),
test_y = diagnosis$diagnosis_test,
groups = groups_return(group_penalty, kth_fold_scores$group_numbers),
plotting = FALSE
)
# Save the results for the current fold
saveRDS(fold_results[[paste0(k, "th_fold")]], path_fold_save_file)
# Completion message for each fold
cat(red(sprintf("Fold %d has completed.\n", k)))
}
# Find intersection of available parameter combinations across all folds
for (k in seq_along(fold_results)) {
if (k == 1) {
combinations = names(fold_results[[k]])
} else {
combinations = intersect(combinations, names(fold_results[[k]]))
}
}
# Filter results to only keep intersecting combinations
fold_results_filtered = lapply(fold_results, function(x) x[names(x) %in% combinations])
# Calculate summary metrics
results = extract_summary_metrics(fold_results_filtered)
final_results = list(fitting_results = fold_results_filtered, metrics = results)
# Save final results
if (!is.null(path_save)) {
# Save final results
saveRDS(final_results, path_final_results)
# Print completion message
cat(green("Final results have been successfully saved to:"), path_final_results, "\n")
}
# Delete temporary fold result files
unlink(path_fold_save, recursive = TRUE, force = TRUE)
return(final_results)
}
subjects[[paste0("Fold_", k, "_Train")]]
# Sub list
sub_train = subjects[[paste0("Fold_", k, "_Train")]]
sub_valid = subjects[[paste0("Fold_", k, "_Validation")]]
# Extract Diagnosis
diagnosis = extract_dignosis(
sub_train = sub_train,
sub_test = sub_valid,
which_group_positive = ifelse(grepl("AD", target_groups), "Dementia",
ifelse(grepl("MCI", target_groups), "MCI", "CN"))
)
# Extract PC scores
kth_fold_scores = extract_fpca_scores_and_group_numbers(path_fold[k])
path_fold[k]
k
fold_results$metrics$ROC_AUC$Mean %>% max
# Find Optimal Combination
metrics = fold_results$metrics
names(metrics)
metrics$F1_Score
# Convert probabilities to binary predictions using 0.5 threshold
pred_class = ifelse(pred_prob >= 0.5, 1, 0)
pred_class
# Calculate Precision, Recall, and F1 Score
tp = sum(pred_class == 1 & test_y == 1)
t
# Save results
results_list = list()
results_list[["y_real"]] = test_y
results_list[["hyper-para_lambda"]] = lambda
# Model fitting using grpreg
if(is.null(lambda)){
results_list[["fitted_model"]] = fit = grpreg(X = as.matrix(train_X),
y = train_y,
group = groups,
penalty = "grLasso",
family = "binomial",
alpha = alpha)
}else{
results_list[["fitted_model"]] = fit = grpreg(X = as.matrix(train_X),
y = train_y,
group = groups,
penalty = "grLasso",
family = "binomial",
lambda = lambda,
alpha = alpha)
}
# Prediction on test set
results_list[["pred_prob"]] = pred_prob = as.numeric(predict(fit, as.matrix(test_X), type = "response"))
length(unique(pred_prob)) != 1
pred_prob
# Find Optimal Combination
metrics = fold_results$metrics
fold_results$metrics$ROC_AUC$Mean %>% max
fold_results$metrics$PR_AUC$Mean %>% max
names(metrics)
fold_results$metrics$ROC_AUC$Mean %>% max
fold_results$metrics$PR_AUC$Mean %>% max
ROC_AUC_max = fold_results$metrics$ROC_AUC$Mean %>% max
PR_AUC_max = fold_results$metrics$PR_AUC$Mean %>% max
# Find Optimal Combination
metrics = fold_results$metrics
F1_Score = metrics$F1_Score
F1_Score_max = metrics$F1_Score %>% max
ind_ROC_AUC_max = which(metrics$ROC_AUC == ROC_AUC_max)
ind_ROC_AUC_max
selected_ROC_AUC_max = ROC_AUC_max[ind_ROC_AUC_max, ]
selected_ROC_AUC_max
ind_ROC_AUC_max = which(metrics$ROC_AUC == ROC_AUC_max)
ROC_AUC_max = metrics$ROC_AUC$Mean %>% max
ind_ROC_AUC_max = which(metrics$ROC_AUC == ROC_AUC_max)
selected_ROC_AUC_max = ROC_AUC_max[ind_ROC_AUC_max, ]
ind_ROC_AUC_max
metrics$ROC_AUC
ind_ROC_AUC_max = which(metrics$ROC_AUC$Mean == ROC_AUC_max)
selected_ROC_AUC_max = ROC_AUC_max[ind_ROC_AUC_max, ]
ind_ROC_AUC_max
metrics$ROC_AUC$Mean
ind_ROC_AUC_max = which(metrics$ROC_AUC$Mean == ROC_AUC_max)
ind_ROC_AUC_max
ROC_AUC_max[ind_ROC_AUC_max, ]
# ROC AUC
ROC_AUC_mean = metrics$ROC_AUC$Mean
ROC_AUC_mean_max = ROC_AUC_mean %>% max
# ROC AUC
ROC_AUC_mean = metrics$ROC_AUC$Mean
ROC_AUC_mean_max = ROC_AUC_mean %>% max
ind_ROC_AUC_mean_max = which(ROC_AUC_mean == ROC_AUC_mean_max)
ind_ROC_AUC_mean_max
selected_ROC_AUC_max = metrics$ROC_AUC[ind_ROC_AUC_mean_max, ]
selected_ROC_AUC_max
View(selected_ROC_AUC_max)
selected_ROC_AUC_max
which.min(selected_ROC_AUC_max$SD)
selected_ROC_AUC_max = metrics$ROC_AUC[ind_ROC_AUC_mean_max, ]
selected_ROC_AUC_max = selected_ROC_AUC_max[which.min(selected_ROC_AUC_max$SD), ]
selected_ROC_AUC_max
# ROC AUC
ROC_AUC_mean = metrics$ROC_AUC$Mean
ROC_AUC_mean_max = ROC_AUC_mean %>% max
ind_ROC_AUC_mean_max = which(ROC_AUC_mean == ROC_AUC_mean_max)
selected_ROC_AUC_max = metrics$ROC_AUC[ind_ROC_AUC_mean_max, ]
if(length(ind_ROC_AUC_mean_max) > 1){
selected_ROC_AUC_max = selected_ROC_AUC_max[which.min(selected_ROC_AUC_max$SD), ]
}
# PR AUC
PR_AUC_mean = metrics$PR_AUC$Mean
# PR AUC
PR_AUC_mean = metrics$PR_AUC$Mean
PR_AUC_mean_max = PR_AUC_mean %>% max
PR_AUC_mean_max
ind_PR_AUC_mean_max = which(PR_AUC_mean == PR_AUC_mean_max)
ind_PR_AUC_mean_max
selected_PR_AUC_max = metrics$PR_AUC[ind_PR_AUC_mean_max, ]
selected_PR_AUC_max
# PR AUC
PR_AUC_mean = metrics$PR_AUC$Mean
PR_AUC_mean_max = PR_AUC_mean %>% max
ind_PR_AUC_mean_max = which(PR_AUC_mean == PR_AUC_mean_max)
selected_PR_AUC_max = metrics$PR_AUC[ind_PR_AUC_mean_max, ]
if(length(ind_PR_AUC_mean_max) > 1){
selected_PR_AUC_max = selected_PR_AUC_max[which.min(selected_PR_AUC_max$SD), ]
}
selected_PR_AUC_max
basename(path_data)
penalized_logistic_grid_fold = function(path_data,
path_splitted_subjects,
group_penalty = TRUE,
alphas,
lambdas,
path_save = NULL){
# Define final results save path
path_final_results = file.path(path_save, basename(path_data), paste0("Classification",
ifelse(group_penalty, "_GroupPen", "_Pen", "_Train"),
".rds"))
# Check if final results file already exists; if so, load and return
if (file.exists(path_final_results)) {
cat(green("Final results file already exists. Loading saved results.\n"))
return(readRDS(path_final_results))
}
target_groups = strsplit(basename(path_data), "___")[[1]][1]
path_fold = list.files(path_data, pattern = "Fold_", full.names = TRUE)
# Load subjects list
subjects = file.path(path_splitted_subjects, target_groups) %>%
list.files(pattern = "train_seed", full.names = TRUE) %>%
readRDS()
fold_results = list()
path_fold_save = file.path(path_save, basename(path_data))
dir.create(path_fold_save, showWarnings = FALSE, recursive = TRUE)
# Model fitting
for (k in seq_along(path_fold)) {
# Define the save path for each fold result
path_fold_save_file = file.path(path_fold_save, paste0(k, "th_fold.rds"))
# Check if fold result file exists; if so, load and skip
if (file.exists(path_fold_save_file)) {
fold_results[[paste0(k, "th_fold")]] = readRDS(path_fold_save_file)
cat(green(sprintf("Fold %d already exists. Loaded saved results.\n", k)))
next  # Skip to the next iteration
}
# Sub list
sub_train = subjects[[paste0("Fold_", k, "_Train")]]
sub_valid = subjects[[paste0("Fold_", k, "_Validation")]]
# Extract Diagnosis
diagnosis = extract_dignosis(
sub_train = sub_train,
sub_test = sub_valid,
which_group_positive = ifelse(grepl("AD", target_groups), "Dementia",
ifelse(grepl("MCI", target_groups), "MCI", "CN"))
)
# Extract PC scores
kth_fold_scores = extract_fpca_scores_and_group_numbers(path_fold[k])
# Define groups if group_penalty is enabled
groups_return = function(group_penalty, group_numbers) {
if (group_penalty) return(group_numbers) else return(NULL)
}
# Logistic regression with penalized grid search
fold_results[[paste0(k, "th_fold")]] = penalized_logistic_grid(
train_X = kth_fold_scores$train_scores %>% dplyr::select(-RID) %>% as.matrix(),
train_y = diagnosis$diagnosis_train,
alphas = alphas,
lambdas = lambdas,
test_X = kth_fold_scores$test_scores %>% dplyr::select(-RID) %>% as.matrix(),
test_y = diagnosis$diagnosis_test,
groups = groups_return(group_penalty, kth_fold_scores$group_numbers),
plotting = FALSE
)
# Save the results for the current fold
saveRDS(fold_results[[paste0(k, "th_fold")]], path_fold_save_file)
# Completion message for each fold
cat(red(sprintf("Fold %d has completed.\n", k)))
}
# Find intersection of available parameter combinations across all folds
for (k in seq_along(fold_results)) {
if (k == 1) {
combinations = names(fold_results[[k]])
} else {
combinations = intersect(combinations, names(fold_results[[k]]))
}
}
# Filter results to only keep intersecting combinations
fold_results_filtered = lapply(fold_results, function(x) x[names(x) %in% combinations])
# Calculate summary metrics
results = extract_summary_metrics(fold_results_filtered)
final_results = list(fitting_results = fold_results_filtered, metrics = results)
# Save final results
if (!is.null(path_save)) {
# Save final results
saveRDS(final_results, path_final_results)
# Print completion message
cat(green("Final results have been successfully saved to:"), path_final_results, "\n")
}
# Delete temporary fold result files
unlink(path_fold_save, recursive = TRUE, force = TRUE)
return(final_results)
}
F1_Score_max = metrics$F1_Score %>% max
fold_results$fitting_results[[1]] %>% dim
fold_results$fitting_results[[1]] %>% length
hyperparameters_combination = fold_results$fitting_results[[1]] %>% names
hyperparameters_combination
create_alpha_lambda_dataframe(hyperparameters_combination)
create_alpha_lambda_dataframe <- function(alpha_lambda_pairs) {
# ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„±
df <- data.frame(matrix(nrow = 0, ncol = length(alpha_lambda_pairs)))
# ê° (alpha, lambda) ìŒì„ ì‚¬ìš©í•˜ì—¬ ì—´ ì´ë¦„ ì„¤ì •
colnames(df) <- sapply(alpha_lambda_pairs, function(pair) {
paste0("alpha_", format(pair$alpha, nsmall = 4),
"_lambda_", format(pair$lambda, nsmall = 4))
})
return(df)
}
create_alpha_lambda_dataframe(hyperparameters_combination)
hyperparameters_combination = fold_results$fitting_results[[1]] %>% names
hyperparameters_combination
alpha_lambda_pairs = hyperparameters_combination
alpha_lambda_pairs
# ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„±
df <- data.frame(matrix(nrow = 0, ncol = length(alpha_lambda_pairs)))
df
alpha_lambda_pairs
# ì•ŒíŒŒì™€ ëŒë‹¤ ê°’ì„ ì¶”ì¶œí•˜ì—¬ ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
create_alpha_lambda_dataframe <- function(input_vector) {
# ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ alphaì™€ lambda ê°’ì„ ì¶”ì¶œ
alpha_values <- as.numeric(sub(".*alpha_([0-9.]+)_lambda_.*", "\\1", input_vector))
lambda_values <- as.numeric(sub(".*lambda_([0-9.]+)", "\\1", input_vector))
# ë°ì´í„°í”„ë ˆì„ ìƒì„±
df <- data.frame(alpha = alpha_values, lambda = lambda_values)
return(df)
}
create_alpha_lambda_dataframe(hyperparameters_combination)
create_alpha_lambda_dataframe(hyperparameters_combination) %>% head
hyper_parameters_df = create_alpha_lambda_dataframe(hyperparameters_combination)
